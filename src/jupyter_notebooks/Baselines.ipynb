{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 138493\n",
      "Number of unique movies: 26744\n"
     ]
    }
   ],
   "source": [
    "# Read the ratings csv file into a pandas Dataframe\n",
    "filename = '../../data/ml-20m/ratings.csv'\n",
    "# filename = '../../data/ml-latest-small/ratings.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "n_users = df['userId'].unique().shape[0]\n",
    "n_items = df['movieId'].unique().shape[0]\n",
    "print(\"Number of unique users: %d\" % n_users)\n",
    "print(\"Number of unique movies: %d\" % n_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    \"\"\" Split the data into training, validation and test partitions by random sampling.\n",
    "\n",
    "        80% of the data is randomly sampled to be the training partition.\n",
    "        10% is held out as a validation dataset to tune the hyperparameters.\n",
    "        10% is held out as a test partition to test the final performance of the model.\n",
    "\n",
    "        Args\n",
    "            df: pandas dataframe object containing the dataset\n",
    "\n",
    "        Returns\n",
    "            df_train: Dataframe corresponding to training partition\n",
    "            df_valid: Dataframe corresponding to validation partition\n",
    "            df_test: Dataframe corresponding to test partition\n",
    "    \"\"\"\n",
    "    random_seed = 1\n",
    "    df_train = df.sample(frac=0.8, random_state=random_seed)\n",
    "    df_rem = df.loc[~df.index.isin(df_train.index)]\n",
    "    df_valid = df_rem.sample(frac=0.5, random_state=random_seed)\n",
    "    df_test = df_rem.loc[~df_rem.index.isin(df_valid.index)]\n",
    "#     logger.info(\"Shape of training dataframe: \" + str(df_train.shape))\n",
    "#     logger.info(\"Shape of validation dataframe: \" + str(df_valid.shape))\n",
    "#     logger.info(\"Sahpe of test dataframe: \" + str(df_test.shape))\n",
    "\n",
    "    return df_train, df_valid, df_test\n",
    "\n",
    "\n",
    "def create_sparse_coo_matrix(df, n_users, n_items, movie_dict):\n",
    "    \"\"\" Create a scipy sparse coo matrix from the given dataframe \n",
    "\n",
    "        Args\n",
    "            df: Dataframe object to be converted to sparse matrix\n",
    "            n_users: Number of rows in the sparse matrix\n",
    "            n_items: Number of columns in the sparse matrix\n",
    "            movie_dict: Dictionary mapping the movies in the dataset to a movie id\n",
    "\n",
    "        Returns\n",
    "            sparse_matrix_coo (scipy.sparse.coo_matrix): Sparse matrix in COO form  \n",
    "    \"\"\"\n",
    "\n",
    "    # Map the movie_ids in the data to the new movie_ids given by the dictionary movie_dict\n",
    "    movie_id_list = list(map(lambda x: movie_dict[x], df['movieId'].tolist()))\n",
    "    # Map the user_id in the dataframe to userid - 1 [to account for zero based indexing]\n",
    "    user_id_list = list(map(lambda x: x - 1, df['userId'].tolist()))\n",
    "    sparse_matrix_coo = sparse.coo_matrix((df['rating'].tolist(),(user_id_list, movie_id_list)),shape=(n_users,n_items))\n",
    "#     logger.debug(\"Shape of created sparse matrix: \" + str(sparse_matrix_coo.shape))\n",
    "#     logger.debug(\"Number of non_zero elements in the sparse matrix: \" + str(sparse_matrix_coo.nnz))\n",
    "#     logger.debug(\"Number of entries in the input dataframe:[should match the number of non zero entries in sparse matrix] \" + str(df.shape[0]))\n",
    "    return sparse_matrix_coo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train, df_valid, df_test = split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# README file for the dataset: http://files.grouplens.org/datasets/movielens/ml-20m-README.html\n",
    "# User-ids are in the range (1, 138493). We just subract 1 from each userId to convert the range to (0,138492)\n",
    "# Total number of movies are 27278 but the the range of movieIds is bigger than (1,27278)\n",
    "# We need to map the movieIds to the range (0,27277)\n",
    "# Only movies with at least one rating or tag are included in the dataset. As we see above, the number of unique movies\n",
    "# for which we have atleast one rating is 26744 \n",
    "ind = 0\n",
    "movie_list = [] # List which is reverse of movie_dict, contains original movieId at index 'new id'\n",
    "movie_dict = {}   # Dictionary from original movieId to new id\n",
    "for movieId in df['movieId'].unique():\n",
    "    movie_list.append(movieId)\n",
    "    movie_dict[movieId] = ind\n",
    "    ind += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sparse matrix for the training, validation and test data\n",
    "sparse_train_coo = create_sparse_coo_matrix(df_train, n_users, n_items, movie_dict)\n",
    "sparse_valid_coo = create_sparse_coo_matrix(df_valid, n_users, n_items, movie_dict)\n",
    "sparse_test_coo = create_sparse_coo_matrix(df_test, n_users, n_items, movie_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrices to CSR form\n",
    "sparse_train_csr = sparse_train_coo.tocsr()\n",
    "sparse_valid_csr = sparse_valid_coo.tocsr()\n",
    "sparse_test_csr = sparse_test_coo.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2000026,)\n"
     ]
    }
   ],
   "source": [
    "# Ignore\n",
    "valid_data = sparse_valid_csr.data\n",
    "print(type(valid_data))\n",
    "print(valid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Average Baseline: Validation MSE: 1.10685057249\n",
      "Global Average Baseline: Test MSE: 1.10811437239\n"
     ]
    }
   ],
   "source": [
    "# Global Average\n",
    "global_avg = sparse_train_csr.sum()/sparse_train_csr.nnz\n",
    "\n",
    "actual_valid = sparse_valid_csr.data\n",
    "pred_valid = np.ones(actual_valid.shape)\n",
    "pred_valid *= global_avg\n",
    "\n",
    "valid_mse = mean_squared_error(pred_valid, actual_valid)\n",
    "\n",
    "actual_test = sparse_test_csr.data\n",
    "pred_test = np.ones(actual_test.shape)\n",
    "pred_test *= global_avg\n",
    "\n",
    "test_mse = mean_squared_error(pred_test, actual_test)\n",
    "\n",
    "print(\"Global Average Baseline: Validation MSE: \" + str(valid_mse))\n",
    "print(\"Global Average Baseline: Test MSE: \" + str(test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138493,)\n",
      "[ 3.70143885  3.96        4.13548387  3.65217391  4.29411765  3.63157895\n",
      "  3.31390135  3.76271186  3.11111111  3.93333333]\n",
      "User Average Baseline: Validation MSE: 0.930433018601\n",
      "User Average Baseline: Test MSE: 0.930988017135\n"
     ]
    }
   ],
   "source": [
    "# User Average Baseline\n",
    "data = sparse_train_csr.data\n",
    "indices = sparse_train_csr.indices\n",
    "indptr = sparse_train_csr.indptr\n",
    "\n",
    "user_sum = sparse_train_csr.sum(axis=1)\n",
    "\n",
    "data_useravg = np.empty(shape=(indptr.shape[0] - 1,),dtype=np.float64)\n",
    "for user_num in range(indptr.shape[0] - 1):\n",
    "    data_useravg[user_num] = user_sum[user_num,0] / (indptr[user_num + 1] - indptr[user_num] + 1e-9)\n",
    "    \n",
    "print(data_useravg.shape)\n",
    "print(data_useravg[:10])\n",
    "\n",
    "indptr_valid = sparse_valid_csr.indptr\n",
    "pred_valid_ua = np.empty(shape=actual_valid.shape,dtype=np.float64)\n",
    "\n",
    "for user_num in range(indptr_valid.shape[0] - 1):\n",
    "    pred_valid_ua[indptr_valid[user_num]: indptr_valid[user_num + 1]] = data_useravg[user_num]\n",
    "    \n",
    "indptr_test = sparse_test_csr.indptr\n",
    "pred_test_ua = np.empty(shape=actual_test.shape,dtype=np.float64)\n",
    "\n",
    "for user_num in range(indptr_test.shape[0] - 1):\n",
    "    pred_test_ua[indptr_test[user_num]: indptr_test[user_num + 1]] = data_useravg[user_num]\n",
    "\n",
    "\n",
    "ua_valid_mse = mean_squared_error(pred_valid_ua, actual_valid)\n",
    "ua_test_mse = mean_squared_error(pred_test_ua, actual_test)\n",
    "\n",
    "print(\"User Average Baseline: Validation MSE: \" + str(ua_valid_mse))\n",
    "print(\"User Average Baseline: Test MSE: \" + str(ua_test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26745,)\n",
      "(26744,)\n",
      "[ 3.20721102  3.94608632  3.89419012  4.05198206  4.33258537  3.4085321\n",
      "  3.53753338  3.86516616  3.49412456  4.18969115]\n",
      "Movie Average Baseline: Validation MSE: 0.888472832341\n",
      "Movie Average Baseline: Test MSE: 0.889690036491\n"
     ]
    }
   ],
   "source": [
    "# Movie Average Baseline\n",
    "sparse_train_transpose = sparse_train_coo.transpose().tocsr()\n",
    "data = sparse_train_transpose.data\n",
    "indices = sparse_train_transpose.indices\n",
    "indptr = sparse_train_transpose.indptr\n",
    "\n",
    "print(indptr.shape)\n",
    "\n",
    "movie_sum = sparse_train_transpose.sum(axis=1)\n",
    "\n",
    "movieavg = np.empty(shape=(indptr.shape[0] - 1,),dtype=np.float64)\n",
    "for movie in range(indptr.shape[0] - 1):\n",
    "    movieavg[movie] = movie_sum[movie,0] / (indptr[movie + 1] - indptr[movie] + 1e-9)\n",
    "    \n",
    "print(movieavg.shape)\n",
    "print(movieavg[:10])\n",
    "\n",
    "pred_valid_ma = []\n",
    "for j in sparse_valid_coo.col:\n",
    "    pred_valid_ma.append(movieavg[j])\n",
    "    \n",
    "pred_test_ma = []\n",
    "for j in sparse_test_coo.col:\n",
    "    pred_test_ma.append(movieavg[j])\n",
    "    \n",
    "actual_valid_ma = sparse_valid_coo.data\n",
    "actual_test_ma = sparse_test_coo.data\n",
    "    \n",
    "ma_valid_mse = mean_squared_error(pred_valid_ma, actual_valid_ma)\n",
    "ma_test_mse = mean_squared_error(pred_test_ma, actual_test_ma)\n",
    "\n",
    "print(\"Movie Average Baseline: Validation MSE: \" + str(ma_valid_mse))\n",
    "print(\"Movie Average Baseline: Test MSE: \" + str(ma_test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.175892    0.43445315  0.60993702  0.12662706  0.76857079]\n",
      "[-0.31833583  0.42053947  0.36864327  0.52643521  0.80703852]\n",
      "Adjusted Average Baseline: Validation MSE: 0.775389573398\n",
      "Adjusted Average Baseline: Test MSE: 0.775445696486\n"
     ]
    }
   ],
   "source": [
    "# Adjusted Average Method\n",
    "ubias = data_useravg - global_avg\n",
    "mbias = movieavg - global_avg\n",
    "\n",
    "print(ubias[:5])\n",
    "print(mbias[:5])\n",
    "\n",
    "pred_valid_aa = []\n",
    "for (i,j) in zip(sparse_valid_coo.row, sparse_valid_coo.col):\n",
    "    pred_valid_aa.append(global_avg + ubias[i] + mbias[j])\n",
    "    \n",
    "pred_test_aa = []\n",
    "for (i,j) in zip(sparse_test_coo.row, sparse_test_coo.col):\n",
    "    pred_test_aa.append(global_avg + ubias[i] + mbias[j])\n",
    "    \n",
    "# print(pred_valid_aa[:10])\n",
    "# print(actual_valid[:10])\n",
    "# print(pred_test_aa[:10])\n",
    "    \n",
    "aa_valid_mse = mean_squared_error(pred_valid_aa, actual_valid_ma)\n",
    "aa_test_mse = mean_squared_error(pred_test_aa, actual_test_ma)\n",
    "\n",
    "print(\"Adjusted Average Baseline: Validation MSE: \" + str(aa_valid_mse))\n",
    "print(\"Adjusted Average Baseline: Test MSE: \" + str(aa_test_mse))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
