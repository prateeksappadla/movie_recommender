{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains efficient implementations of the Collaborative Filtering techniques. This implementation makes use of sparse representations of the matrices. The algorithms take very long to implement on the larger version of the dataset. In this notebook, we demonstrate the use of these algos on the smaller version of the dataset. The code is also available as .py files which can be used on the larger version of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 671\n",
      "Number of unique movies: 9066\n"
     ]
    }
   ],
   "source": [
    "# filename = '../../data/ml-20m/ratings.csv'\n",
    "filename = '../../data/ml-latest-small/ratings.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "n_users = df['userId'].unique().shape[0]\n",
    "n_items = df['movieId'].unique().shape[0]\n",
    "print(\"Number of unique users: %d\" % n_users)\n",
    "print(\"Number of unique movies: %d\" % n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90004, 4)\n",
      "(10000, 4)\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sparse\n",
    "\n",
    "# Split the data into two parts, train and test. Randomly sample 10% of the entries in the csv file as test data and \n",
    "# the rest of the data will be used as training data\n",
    "\n",
    "# Also create a validation partition for tuning the hyperparameters\n",
    "df_train = df.sample(frac=0.9)\n",
    "df_test = df.loc[~df.index.isin(df_train.index)]\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9066,)\n",
      "9066\n"
     ]
    }
   ],
   "source": [
    "# README file for the dataset: http://files.grouplens.org/datasets/movielens/ml-20m-README.html\n",
    "# User-ids are in the range (1, 138493). We just subract 1 from each userId to convert the range to (0,138492)\n",
    "# Total number of movies are 27278 but the the range of movieIds is bigger than (1,27278)\n",
    "# We need to map the movieIds to the range (0,27277)\n",
    "# Only movies with at least one rating or tag are included in the dataset. As we see above, the number of unique movies\n",
    "# for which we have atleast one rating is 26744 \n",
    "\n",
    "print(df['movieId'].unique().shape)\n",
    "ind = 0\n",
    "movie_list = [] # List which is reverse of movie_dict, contains original movieId at index 'new id'\n",
    "movie_dict = {}   # Dictionary from original movieId to new id\n",
    "for movieId in df['movieId'].unique():\n",
    "    movie_list.append(movieId)\n",
    "    movie_dict[movieId] = ind\n",
    "    ind += 1\n",
    "\n",
    "print(len(movie_list))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90004\n",
      "90004\n",
      "90004\n"
     ]
    }
   ],
   "source": [
    "# Create sparse matrix for the training data\n",
    "print(len(df_train['movieId'].tolist()))\n",
    "\n",
    "movie_id_list = list(map(lambda x: movie_dict[x], df_train['movieId'].tolist()))\n",
    "print(len(movie_id_list))\n",
    "user_id_list = list(map(lambda x: x - 1, df_train['userId'].tolist()))\n",
    "print(len(user_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Create sparse matrix for the test data\n",
    "print(len(df_test['movieId'].tolist()))\n",
    "\n",
    "test_movie_id_list = list(map(lambda x: movie_dict[x], df_test['movieId'].tolist()))\n",
    "#print(len(test_movie_id_list))\n",
    "test_user_id_list = list(map(lambda x: x - 1, df_test['userId'].tolist()))\n",
    "#print(len(test_user_id_list))\n",
    "\n",
    "sparse_test = sparse.coo_matrix((df_test['rating'].tolist(), (test_user_id_list,test_movie_id_list)),shape=(n_users,n_items))\n",
    "print(sparse_test.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(671, 9066)\n",
      "90004\n"
     ]
    }
   ],
   "source": [
    "sparse_train = sparse.coo_matrix((df_train['rating'].tolist(),(user_id_list, movie_id_list)),shape=(n_users,n_items))\n",
    "print(sparse_train.shape)\n",
    "print(sparse_train.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(671, 1)\n"
     ]
    }
   ],
   "source": [
    "user_sum = sparse_train.sum(axis=1)\n",
    "print(user_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(90004,)\n",
      "(672,)\n"
     ]
    }
   ],
   "source": [
    "sparse_train_csr = sparse_train.tocsr()\n",
    "data = sparse_train_csr.data\n",
    "indices = sparse_train_csr.indices\n",
    "indptr = sparse_train_csr.indptr\n",
    "print(type(data))\n",
    "print(type(indices))\n",
    "print(type(indptr))\n",
    "print(indices.shape)\n",
    "print(indptr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90004,)\n",
      "[ 2.5         2.5         2.5         2.5         2.5         2.5         2.5\n",
      "  2.5         2.5         2.5         2.5         2.5         2.5         2.5\n",
      "  2.5         2.5         3.51470588  3.51470588  3.51470588  3.51470588\n",
      "  3.51470588  3.51470588  3.51470588  3.51470588  3.51470588]\n",
      "[ 2.5  3.   3.   2.   4.   2.   2.   3.5  2.   2.5] [ 2.5  2.5  2.5  2.5  2.5  2.5  2.5  2.5  2.5  2.5] [ 0.   0.5  0.5 -0.5  1.5 -0.5 -0.5  1.  -0.5  0. ]\n"
     ]
    }
   ],
   "source": [
    "data_useravg = np.empty(shape=data.shape,dtype=np.float64)\n",
    "print(data_useravg.shape)\n",
    "\n",
    "# Compute the average rating for each user\n",
    "for user_num in range(indptr.shape[0] - 1):\n",
    "    data_useravg[indptr[user_num]: indptr[user_num + 1]] = user_sum[user_num,0] / (indptr[user_num + 1] - indptr[user_num])\n",
    "    \n",
    "print(data_useravg[:25]) \n",
    "\n",
    "data_centered = data - data_useravg\n",
    "print(data[:10] , data_useravg[:10], data_centered[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90004\n",
      "(671, 9066)\n"
     ]
    }
   ],
   "source": [
    "# Create sparse CSR matrix of centered user data\n",
    "sparse_train_ucentered = sparse.csr_matrix((data_centered,indices,indptr),shape=(n_users,n_items))\n",
    "\n",
    "print(sparse_train_ucentered.nnz)\n",
    "print(sparse_train_ucentered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def latent_factor(sparse_ratings_coo, k=500, lambda1=1, lambda2=1, epochs=50, lr=0.1):\n",
    "    \n",
    "    # Initialize the latent factor matrices randomly\n",
    "    Q = np.random.uniform(0,0.1,size=(sparse_ratings_coo.shape[0],k))\n",
    "    P = np.random.uniform(0,0.1,size=(sparse_ratings_coo.shape[1],k))\n",
    "    \n",
    "    for ep in range(1,epochs+1):\n",
    "        print(\"Epoch \" + str(ep))\n",
    "        for i,j,v in zip(sparse_ratings_coo.row, sparse_ratings_coo.col, sparse_ratings_coo.data):\n",
    "            err = v - Q[i,:].dot(P[j,:])\n",
    "            \n",
    "            grad_Q = 2 * (lambda2 * Q[i,:] - (err)*P[j,:])  \n",
    "            grad_P = 2 * (lambda1 * P[j,:] - (err)*Q[i,:])\n",
    "            \n",
    "            Q[i,:] -= lr * grad_Q\n",
    "            P[j,:] -= lr * grad_P                                           \n",
    "                                                       \n",
    "    return Q,P    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def latent_factor_predict(sparse_test_coo, Q, P):\n",
    "    \"\"\" Get the ratings for the test data points using the learnt Q and P matrices\n",
    "    \"\"\"\n",
    "    pred = []\n",
    "    \n",
    "    for i,j in zip(sparse_test_coo.row, sparse_test_coo.col):\n",
    "        pred.append(Q[i,:].dot(P[j,:]))\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n"
     ]
    }
   ],
   "source": [
    "Q,P = latent_factor(sparse_train,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.45798708389\n"
     ]
    }
   ],
   "source": [
    "pred = latent_factor_predict(sparse_test, Q, P)\n",
    "actual = sparse_test.data\n",
    "mse = mean_squared_error(pred,actual)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "  (1, 0)\t0.192160153478\n",
      "  (1, 4)\t0.931613711497\n",
      "  (8, 2)\t0.899914321873\n",
      "  (6, 4)\t0.927588282635\n",
      "  (2, 1)\t0.544007712114\n",
      "  (4, 4)\t0.2008087566\n",
      "  (4, 2)\t0.310863129116\n",
      "  (4, 3)\t0.610998632784\n",
      "  (0, 0)\t0.336660192172\n",
      "  (9, 1)\t0.160311436188\n",
      "  (2, 5)\t0.0465962755326\n",
      "  (4, 0)\t0.281276183295\n",
      "  (3, 2)\t0.331769992642\n",
      "  (2, 3)\t0.462017330272\n",
      "  (6, 6)\t0.177778118874\n",
      "  (2, 4)\t0.400253235155\n",
      "  (6, 1)\t0.945580065076\n"
     ]
    }
   ],
   "source": [
    "# Generate a toy sparse matrix for trying out methods\n",
    "\n",
    "trial_mat_coo = sparse.random(10,7,density=0.25)\n",
    "print(trial_mat_coo.nnz)\n",
    "print(trial_mat_coo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7)\n"
     ]
    }
   ],
   "source": [
    "trial_mat_csr = trial_mat_coo.tocsr()\n",
    "sample_row = trial_mat_csr.getrow(1).toarray()\n",
    "print(sample_row.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t0.864153962726\n",
      "  (0, 2)\t0.372881401941\n",
      "  (0, 4)\t0.241126265589\n",
      "  (0, 1)\t0.904829632034\n",
      "  (0, 0)\t0.0646926741975\n",
      "(1, 10)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "similarity = sample_row.dot(trial_mat_csr.transpose())\n",
    "print(similarity)\n",
    "print(similarity.shape)\n",
    "print(type(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.0646926741975\n",
      "  (0, 1)\t0.904829632034\n",
      "  (0, 2)\t0.372881401941\n",
      "  (0, 4)\t0.241126265589\n",
      "  (0, 6)\t0.864153962726\n"
     ]
    }
   ],
   "source": [
    "similarity.sort_indices() # Sorts indices, not the values\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.matrixlib.defmatrix.matrix'>\n",
      "(1, 10)\n",
      "[[ 0.06469267  0.90482963  0.3728814   0.          0.24112627  0.\n",
      "   0.86415396  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "sim_dense = similarity.todense()\n",
    "print(type(sim_dense))\n",
    "print(sim_dense.shape)\n",
    "print(sim_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06469267  0.90482963  0.3728814   0.          0.24112626  0.\n",
      "   0.86415396  0.          0.          0.        ]]\n",
      "[[ 0.06469267  0.90482963  0.3728814   0.          0.24112626  0.\n",
      "   0.86415396  0.          0.          0.        ]]\n",
      "(1, 10)\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'>\n",
      "[ 0.06469267  0.90482963  0.3728814   0.          0.24112626  0.\n",
      "  0.86415396  0.          0.          0.        ]\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "#norms = np.random.uniform(0,1,size=sim_dense.shape)\n",
    "norms = np.ones(similarity.shape)\n",
    "print(similarity)\n",
    "similarity /= (norms + 1e-9)\n",
    "print(similarity)\n",
    "print(similarity.shape)\n",
    "print(type(similarity))\n",
    "sim_np = np.array(similarity).reshape(-1)\n",
    "print(sim_np)\n",
    "print(sim_np.shape)\n",
    "sim_np.arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_norms(sparse_train_csr):\n",
    "    \"\"\" Compute the norm of the rating vector for each user.\n",
    "        These norm values will be used for normalizing during computation of cosine similarity\n",
    "    \"\"\"\n",
    "    n_users = sparse_train_csr.shape[0]\n",
    "    norms = np.empty(shape=(n_users,),dtype=np.float64)\n",
    "    \n",
    "    for i in range(n_users):\n",
    "        row = sparse_train_csr.getrow(i)\n",
    "        #getrow returns a 1 x n vector. Dot product gives a 1 x 1 numpy array and then we index using [0,0] to get a scalar\n",
    "        norms[i] = np.sqrt(row.dot(row.T))[0,0]\n",
    "    return norms    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_sim(userid, sparse_train_csr, norms):\n",
    "    \"\"\" Compute the similarity between a particular user and all other users\n",
    "        return an array/list containing the similarity with all other users\n",
    "    \"\"\"\n",
    "    row = sparse_train_csr.getrow(userid)\n",
    "    sim = row.dot(sparse_train_csr.transpose())\n",
    "    \n",
    "    sim /= (norms[userid] + 1e-9)\n",
    "    sim /= (norms + 1e-9)\n",
    "    \n",
    "    #Convert from np.matrix datatype to a numpy ndarray, reshape to a vector and then return\n",
    "    return np.array(sim).reshape(-1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_cf(userid, user_avg, user_sim, sparse_train_csr, k=100):\n",
    "    \"\"\" user_sim is a numpy array of shape (n_users,)\n",
    "    \"\"\"\n",
    "    \n",
    "    top_k_users = user_sim.argsort()[-k-1:-1]\n",
    "    top_k_sim = user_sim[top_k_users]\n",
    "    \n",
    "    # Initialize all predictions to the average rating given by that user\n",
    "    pred = np.ones(sparse_train_csr.shape[1])\n",
    "    pred *= user_avg\n",
    "    \n",
    "    #print(pred)\n",
    "    \n",
    "    top_k_ratings = np.empty((k,sparse_train_csr.shape[1]),dtype=np.float64)\n",
    "    for i in range(k):\n",
    "        top_k_ratings[i] = sparse_train_csr.getrow(top_k_users[i]).toarray()\n",
    "        \n",
    "    #print(top_k_ratings)    \n",
    "        \n",
    "    # Map of non-zero rating entries\n",
    "    ratings_map = (top_k_ratings !=0).astype(int)\n",
    "    #print(ratings_map)\n",
    "    \n",
    "    normalizer = top_k_sim.reshape((1,-1)).dot(ratings_map)\n",
    "    #print(normalizer.shape)\n",
    "    \n",
    "    # pred is of shape (n_items,) and the other operand is of shape (1,n_items), hence the indexing [0,:]\n",
    "    pred += (top_k_sim.reshape((1,-1)).dot(top_k_ratings) / (normalizer + 1e-9))[0,:]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test above three methods on a toy example\n",
    "trial_mat_coo = sparse.random(10,7,density=0.25)\n",
    "trial_mat_csr = trial_mat_coo.tocsr()\n",
    "\n",
    "# trial_norms = compute_norms(trial_mat_csr)\n",
    "# print(trial_norms.shape)\n",
    "# print(trial_norms)\n",
    "\n",
    "# trial_usim = user_sim(1, trial_mat_csr, trial_norms)\n",
    "# print(trial_usim.shape)\n",
    "# print(trial_usim)\n",
    "\n",
    "# trial_pred=user_cf(1, 5.0, trial_usim, trial_mat_csr, k=5)\n",
    "# print(trial_pred.shape)\n",
    "# print(trial_pred)\n",
    "\n",
    "trial_test_coo = sparse.random(10,7, density=0.15)\n",
    "trial_test_csr = trial_test_coo.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_usercf_MSE(sparse_train_csr, sparse_test_csr, k=100):    \n",
    "    \"\"\" Compute MSE on the held out test data\n",
    "    \"\"\"\n",
    "    # Compute the average rating for each user\n",
    "    data_useravg = np.empty(shape=data.shape,dtype=np.float64)\n",
    "    for user_num in range(indptr.shape[0] - 1):\n",
    "        data_useravg[indptr[user_num]: indptr[user_num + 1]] = user_sum[user_num,0] / (indptr[user_num + 1] - indptr[user_num])\n",
    "    \n",
    "    norms = compute_norms(sparse_train_csr)    \n",
    "    n_users = sparse_train_csr.shape[0]\n",
    "    \n",
    "    total_mse = 0\n",
    "    \n",
    "    for user in range(n_users):\n",
    "        usim = user_sim(user, sparse_train_csr, norms)\n",
    "        pred = user_cf(user, data_useravg[user], usim, sparse_train_csr,k)\n",
    "        \n",
    "        actual = sparse_test_csr.getrow(user).toarray().squeeze(axis=0)\n",
    "        test_mask = (actual != 0).astype(int)\n",
    "        \n",
    "        pred = pred * test_mask\n",
    "        mse_user = mean_squared_error(pred,actual)\n",
    "        \n",
    "        total_mse += (mse_user * np.count_nonzero(actual))\n",
    "        \n",
    "    return total_mse/sparse_test_csr.nnz             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.74132967815\n"
     ]
    }
   ],
   "source": [
    "trial_mse = compute_usercf_MSE(trial_mat_csr, trial_test_csr)\n",
    "print(trial_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse_test_csr = sparse_test.tocsr()\n",
    "ucf_mse = compute_usercf_MSE(sparse_train_ucentered, sparse_test_csr, 100)\n",
    "print(ucf_mse)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
